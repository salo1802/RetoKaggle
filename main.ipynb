{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIG & HELPERS\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install --quiet torch torchvision torchaudio tensorboard scikit-learn matplotlib pandas\n",
    "\n",
    "import os, math, random, numpy as np, pandas as pd\n",
    "import torch, torchvision, torch.nn as nn, torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "\n",
    "def get_output_shape(model: nn.Module, image_dim: Tuple[int,int,int,int]):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = torch.randn(*image_dim, device=device)\n",
    "        out = model(x)\n",
    "        return tuple(out.shape)\n",
    "\n",
    "def xyxy_to_valid(b: torch.Tensor) -> torch.Tensor:\n",
    "    b = b.float().view(-1,4)\n",
    "    x1 = torch.minimum(b[:,0], b[:,2])\n",
    "    y1 = torch.minimum(b[:,1], b[:,3])\n",
    "    x2 = torch.maximum(b[:,0], b[:,2])\n",
    "    y2 = torch.maximum(b[:,1], b[:,3])\n",
    "    return torch.stack([x1,y1,x2,y2], dim=1)\n",
    "\n",
    "def clamp_01(b: torch.Tensor) -> torch.Tensor:\n",
    "    return b.clamp(0.0, 1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPARATION & EXPLORATION\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_path = Path('fa-ii-2025-ii-object-localization/train.csv')\n",
    "df = pd.read_csv(data_path)\n",
    "print('Samples:', len(df))\n",
    "display(df.head())\n",
    "print(df['class'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA NORMALIZATION & TRANSFORMATIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# train/validation split\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['class'], random_state=42)\n",
    "print(f'Train: {len(train_df)} | Val: {len(val_df)}')\n",
    "\n",
    "# compute normalization stats on training subset only\n",
    "img_dir = Path('fa-ii-2025-ii-object-localization/images')\n",
    "\n",
    "def compute_stats(sub_df):\n",
    "    sums = np.zeros(3); sqs = np.zeros(3); n_pix = 0\n",
    "    for fname in tqdm(sub_df['filename'], desc='compute stats'):\n",
    "        img = np.array(Image.open(img_dir/fname).convert('RGB'), dtype=np.float32) / 255.0\n",
    "        sums += img.reshape(-1,3).sum(0)\n",
    "        sqs  += (img.reshape(-1,3)**2).sum(0)\n",
    "        n_pix += img.shape[0]*img.shape[1]\n",
    "    mean = sums / n_pix\n",
    "    std = np.sqrt(sqs / n_pix - mean**2)\n",
    "    return mean, std\n",
    "\n",
    "mean, std = compute_stats(train_df)\n",
    "print('mean:', mean, 'std:', std)\n",
    "\n",
    "# augmentations\n",
    "h = w = 256\n",
    "train_transforms = A.Compose([\n",
    "    A.Resize(h, w),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ColorJitter(0.2,0.2,0.2,0.1,p=0.5),\n",
    "    A.Normalize(mean=mean, std=std),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_id']))\n",
    "\n",
    "eval_transforms = A.Compose([\n",
    "    A.Resize(h, w),\n",
    "    A.Normalize(mean=mean, std=std),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_id']))\n",
    "\n",
    "class MaskDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = np.array(Image.open(self.root_dir / row['filename']).convert('RGB'))\n",
    "        bbox = [row['xmin'], row['ymin'], row['xmax'], row['ymax']]\n",
    "        class_id = 1 if row['class']=='mask' else 0\n",
    "        if self.transform:\n",
    "            sample = self.transform(image=image, bboxes=[bbox], class_id=[class_id])\n",
    "            image = sample['image']\n",
    "            bbox = sample['bboxes'][0]\n",
    "            class_id = sample['class_id'][0]\n",
    "        return {\n",
    "            'image': image,\n",
    "            'bbox': torch.tensor(bbox, dtype=torch.float32),\n",
    "            'class_id': torch.tensor(class_id, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "batch_size = 16\n",
    "train_root_dir = img_dir\n",
    "train_data = DataLoader(MaskDataset(train_df, train_root_dir, transform=train_transforms),\n",
    "                        batch_size=batch_size, shuffle=True, num_workers=0,\n",
    "                        pin_memory=(device=='cuda'))\n",
    "val_data = DataLoader(MaskDataset(val_df, train_root_dir, transform=eval_transforms),\n",
    "                      batch_size=batch_size, shuffle=False, num_workers=0,\n",
    "                      pin_memory=(device=='cuda'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}